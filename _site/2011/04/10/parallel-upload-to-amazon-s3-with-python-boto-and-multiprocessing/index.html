<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Parallel upload to Amazon S3 with python, boto and multiprocessing &#8211; Blue Collar Bioinformatics</title>

<meta name="keywords" content="infrastructure">

  

<!-- Twitter Cards -->
<meta name="twitter:title" content="Parallel upload to Amazon S3 with python, boto and multiprocessing">

<meta name="twitter:site" content="@chapmanb">
<meta name="twitter:creator" content="@chapmanb">

<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://localhost:4000/images/">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Parallel upload to Amazon S3 with python, boto and multiprocessing">

<meta property="og:url" content="http://localhost:4000/2011/04/10/parallel-upload-to-amazon-s3-with-python-boto-and-multiprocessing/">
<meta property="og:site_name" content="Blue Collar Bioinformatics">





<link rel="canonical" href="http://localhost:4000/2011/04/10/parallel-upload-to-amazon-s3-with-python-boto-and-multiprocessing/">
<link href="http://feeds2.feedburner.com/bcbio" type="application/atom+xml" rel="alternate" title="Blue Collar Bioinformatics Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">
<!-- Webfonts -->
<script src="https://use.edgefonts.net/source-sans-pro:n2,i2,n3,i3,n4,i4,n6,i6,n7,i7,n9,i9;source-code-pro:n4,n7;volkhov.js"></script>

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
  <script src="http://localhost:4000/assets/js/vendor/html5shiv.min.js"></script>
  <script src="http://localhost:4000/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="http://localhost:4000/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>


<!-- MathJax -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://localhost:4000/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://localhost:4000/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://localhost:4000/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/images/apple-touch-icon-144x144-precomposed.png">

</head>

<body id="post">

<div class="navigation-wrapper">
	<nav role="navigation" id="site-nav" class="animated drop">
	    <ul>
      
		    
		        
		    
		    <li><a href="http://localhost:4000/" >About</a></li>
		  
		    
		        
		    
		    <li><a href="http://localhost:4000/articles/" >Recent</a></li>
		  
		    
		        
		    
		    <li><a href="http://localhost:4000/tags/" >Topics</a></li>
		  
		    
		        
		    
		    <li><a href="http://localhost:4000/search/" >Search</a></li>
		  
	    </ul>
	</nav>
</div><!-- /.navigation-wrapper -->

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->

<header class="masthead">
	<div class="wrap">
      
      <h1 class="site-title animated fadeIn"><a href="http://localhost:4000/">Blue Collar Bioinformatics</a></h1>
		<h2 class="site-description animated fadeIn" itemprop="description">Biological data analysis, validation and visualization</h2>
	</div>
</header><!-- /.masthead -->

<div class="js-menu-screen menu-screen"></div>


<div id="main" role="main">
  <article class="hentry">
    
    <div class="entry-wrapper">
      <header class="entry-header">
        <span class="entry-tags"><a href="http://localhost:4000/tags/#infrastructure" title="Pages tagged infrastructure">infrastructure</a></span>
        
          <h1 class="entry-title">Parallel upload to Amazon S3 with python, boto and multiprocessing</h1>
        
      </header>
      <footer class="entry-meta">
        
          
        
          <img src="http://localhost:4000/images/diego-work.jpg" class="bio-photo" alt="Brad Chapman bio photo"></a>
        
        <span class="author vcard">By <span class="fn">Brad Chapman</span></span>
        <span class="entry-date date published"><time datetime="2011-04-10T13:27:10-04:00"><i class="fa fa-calendar-o"></i> April 10, 2011</time></span>
        
        <span class="entry-comments"><i class="fa fa-comment-o"></i> <a href="#disqus_thread">Comment</a></span>
        
        
      </footer>
      <div class="entry-content">
        <p>One challenge with moving analysis pipelines to cloud resources like <a href="http://aws.amazon.com/ec2/">Amazon EC2</a> is figuring out the logistics of transferring files. Biological data is big; with the rapid adoption of new machines like the <a href="http://www.illumina.com/systems/hiseq_2000.ilmn">HiSeq</a> and decreasing <a href="http://www.genome.gov/sequencingcosts/">sequencing costs</a>, the data transfer question isn't going away soon. The use of Amazon in bioinformatics was brought up during a recent <a href="http://biostar.stackexchange.com/questions/7143/is-amazons-ec2-commonly-used-for-bioinformatics">discussion on the BioStar question answer site</a>. <a href="http://mndoci.github.com/">Deepak's</a> answer highlighted the role of parallelizing uploads and downloads to ease this transfer burden. Here I describe a method to improve upload speed by splitting over multiple processing cores.</p>
<p><a href="http://aws.amazon.com/s3/faqs/#What_is_Amazon_S3">Amazon Simple Storage System (S3)</a> provides relatively inexpensive cloud storage with their <a href="http://aws.amazon.com/s3/faqs/#What_is_RRS">reduced redundancy storage</a> option. S3, and all of Amazon's cloud services, are accessible directly from Python using <a href="http://code.google.com/p/boto/">boto</a>. By using <a href="http://www.elastician.com/2010/12/s3-multipart-upload-in-boto.html">boto's multipart upload support</a>, coupled with Python's built in <a href="http://docs.python.org/library/multiprocessing.html">multiprocessing</a> module, I'll demonstrate maximizing transfer speeds to make uploading data less painful. The <a href="https://github.com/chapmanb/cloudbiolinux/blob/master/utils/s3_multipart_upload.py">script is available from GitHub</a> and requires <a href="https://github.com/boto/boto">the latest boto from GitHub (2.0b5 or better)</a>.</p>
<div id="parallel-upload-with-multiprocessing">
<h2>Parallel upload with multiprocessing</h2>
<p>The overall process uses boto to connect to an S3 upload bucket, initialize a multipart transfer, split the file into multiple pieces, and then upload these pieces in parallel over multiple cores. Each processing core is passed a set of credentials to identify the transfer: the multipart upload identifier (<code>mp.id</code>), the S3 file key name (<code>mp.key_name</code>) and the S3 bucket name (<code>mp.bucket_name</code>).</p>
<p>[sourcecode language="python"]<br />
import boto</p>
<p>conn = boto.connect_s3()<br />
bucket = conn.lookup(bucket_name)<br />
mp = bucket.initiate_multipart_upload(s3_key_name, reduced_redundancy=use_rr)<br />
with multimap(cores) as pmap:<br />
    for _ in pmap(transfer_part, ((mp.id, mp.key_name, mp.bucket_name, i, part)<br />
                                  for (i, part) in<br />
                                  enumerate(split_file(tarball, mb_size, cores)))):<br />
        pass<br />
mp.complete_upload()<br />
[/sourcecode]</p>
<p>The <code>split_file</code> function uses the unix split command to divide the file into sections, each of which will be uploaded separately.</p>
<p>[sourcecode language="python"]<br />
def split_file(in_file, mb_size, split_num=5):<br />
    prefix = os.path.join(os.path.dirname(in_file),<br />
                          &quot;%sS3PART&quot; % (os.path.basename(s3_key_name)))<br />
    split_size = int(min(mb_size / (split_num * 2.0), 250))<br />
    if not os.path.exists(&quot;%saa&quot; % prefix):<br />
        cl = [&quot;split&quot;, &quot;-b%sm&quot; % split_size, in_file, prefix]<br />
        subprocess.check_call(cl)<br />
    return sorted(glob.glob(&quot;%s*&quot; % prefix))<br />
[/sourcecode]</p>
<p>The multiprocessing aspect is managed using a <a href="http://docs.python.org/library/contextlib.html">contextmanager</a>. The initial multiprocessing pool is setup, using a specified number of cores, and configured to allow keyboard interrupts. We then return a lazy map function (<a href="http://docs.python.org/library/itertools.html#itertools.imap">imap</a>) which can be used just like Python's standard <code>map</code>. This transparently divides the function calls for each file part over all available cores. Finally, the pool is cleaned up when the map is finished running.</p>
<p>[sourcecode language="python"]<br />
@contextlib.contextmanager<br />
def multimap(cores=None):<br />
    if cores is None:<br />
        cores = max(multiprocessing.cpu_count() - 1, 1)<br />
    def wrapper(func):<br />
        def wrap(self, timeout=None):<br />
            return func(self, timeout=timeout if timeout is not None else 1e100)<br />
        return wrap<br />
    IMapIterator.next = wrapper(IMapIterator.next)<br />
    pool = multiprocessing.Pool(cores)<br />
    yield pool.imap<br />
    pool.terminate()<br />
[/sourcecode]</p>
<p>The actual work of transferring each portion of the file is done using two functions. The helper function, <code>mp_from_ids</code>, uses the id information about the bucket, file key and multipart upload id to reconstitute a multipart upload object:</p>
<p>[sourcecode language="python"]<br />
def mp_from_ids(mp_id, mp_keyname, mp_bucketname):<br />
    conn = boto.connect_s3()<br />
    bucket = conn.lookup(mp_bucketname)<br />
    mp = boto.s3.multipart.MultiPartUpload(bucket)<br />
    mp.key_name = mp_keyname<br />
    mp.id = mp_id<br />
    return mp<br />
[/sourcecode]</p>
<p>This object, together with the number of the file part and the file itself, are used to transfer that section of the file. The file part is removed after successful upload.</p>
<p>[sourcecode language="python"]<br />
@map_wrap<br />
def transfer_part(mp_id, mp_keyname, mp_bucketname, i, part):<br />
    mp = mp_from_ids(mp_id, mp_keyname, mp_bucketname)<br />
    print &quot; Transferring&quot;, i, part<br />
    with open(part) as t_handle:<br />
        mp.upload_part_from_file(t_handle, i+1)<br />
    os.remove(part)<br />
[/sourcecode]</p>
<p>When all sections, distributed over all processors, are finished, the multipart upload is signaled complete and Amazon finishes the process. Your file is now available on S3.</p>
</div>
<div id="parallel-download">
<h2>Parallel download</h2>
<p>Download speeds can be maximized by utilizing several existing parallelized accelerators:</p>
<ul>
<li><a href="http://axel.alioth.debian.org/">axel</a></li>
<li><a href="http://aria2.sourceforge.net/">aria2</a></li>
<li><a href="http://lftp.yar.ru/">lftp</a></li>
</ul>
<p>Combine these with the uploader to build up a cloud analysis workflow: move your data to S3, run a complex analysis pipeline on EC2, push the results back to S3, and then download them to local machines. Please share other tips and tricks you use to deal with Amazon file transfer in the comments.</p>
</div>

        
          <div id="disqus_thread"></div><!-- /#disqus_thread -->
          
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'bcbio'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'https://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

        
      </div><!-- /.entry-content -->
    </div><!-- /.entry-wrapper -->
    <nav class="pagination" role="navigation">
      
        <a href="http://localhost:4000/2011/01/11/next-generation-sequencing-information-management-and-analysis-system-for-galaxy/" class="btn" title="Next generation sequencing information management and analysis system for Galaxy">Previous</a>
      
      
        <a href="http://localhost:4000/2011/04/10/bioinformatics-jobs-at-harvard-school-of-public-health/" class="btn" title="Bioinformatics jobs at Harvard School of Public Health">Next</a>
      
    </nav><!-- /.pagination -->
  </article>
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo" class="entry-wrapper">
    

<div class="social-icons">
	<a href="https://twitter.com/chapmanb" title="Brad Chapman on Twitter" target="_blank"><i class="fa fa-twitter-square fa-2x"></i></a>
	<a href="https://github.com/chapmanb" title="Brad Chapman on Github" target="_blank"><i class="fa fa-github-square fa-2x"></i></a>
	<a href="https://github.com/chapmanb" title="Brad Chapman on Bitbucket" target="_blank"><i class="fa fa-bitbucket-square fa-2x"></i></a>
	
	
	
	
	
	
  
	
	<a href="https://linkedin.com/pub/3/300/8ba" title="Brad Chapman on LinkedIn" target="_blank"><i class="fa fa-linkedin-square fa-2x"></i></a>
  <a href="http://feeds2.feedburner.com/bcbio" title="Atom/RSS feed"><i class="fa fa-rss-square fa-2x"></i></a>
</div><!-- /.social-icons -->

  </footer>
</div><!-- /.footer-wrapper -->

<script type="text/javascript">
  var BASE_URL = 'http://localhost:4000';
</script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://localhost:4000/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://localhost:4000/assets/js/scripts.min.js"></script>




</body>
</html>
