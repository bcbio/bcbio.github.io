---
layout: post
title: Distributed exome analysis pipeline with CloudBioLinux and CloudMan
date: 2011-08-19 17:33:16.000000000 -04:00
categories:
- analysis
tags:
- infrastructure
status: publish
type: post
published: true
meta:
  _edit_last: '6099765'
  tagazine-media: a:7:{s:7:"primary";s:0:"";s:6:"images";a:0:{}s:6:"videos";a:0:{}s:11:"image_count";s:1:"0";s:6:"author";s:7:"6099765";s:7:"blog_id";s:7:"5850073";s:9:"mod_stamp";s:19:"2011-08-19
    21:33:16";}
  _oembed_36589f3c106dcc671cb55943b9115aa8: ! '{{unknown}}'
  _oembed_2f4f6c37516d1f25481f76a41693a659: ! '{{unknown}}'
  _oembed_600c70aa27a2fe18b06fc0bf5d7a33b0: ! '{{unknown}}'
author: brad_chapman
excerpt: !ruby/object:Hpricot::Doc
---
<p>A major challenge in building analysis pipelines for next-generation sequencing data is combining a large number of processing steps in a flexible, scalable manner. Current best-practice software needs to be installed and configured alongside the custom code to chain individual programs together. Scaling to handle increasing throughput requires running that custom code on a wide variety of parallel architectures, from single multicore machines to heterogeneous clusters.</p>
<p>Establishing community resources that meet the challenges of building these pipelines ensures that bioinformatics programmers can share the burden of building large scale systems. Two open-source efforts which aim at providing this type of architecture are:</p>
<ul>
<li>
<p><a href="http://cloudbiolinux.org">CloudBioLinux</a> -- A community effort to create shared images filled with bioinformatics software and libraries, using an automated build environment.</p>
</li>
<li>
<p><a href="http://wiki.g2.bx.psu.edu/Admin/Cloud">CloudMan</a> -- Uses CloudBioLinux as a platform to build a full SGE cluster environment. Written by <a href="http://userwww.service.emory.edu/~eafgan/">Enis Afgan</a> and the <a href="http://wiki.g2.bx.psu.edu/">Galaxy Team</a>, CloudMan is used to provide a ready-to-run, dynamically scalable version of Galaxy on <a href="http://aws.amazon.com/">Amazon AWS</a>.</p>
</li>
</ul>
<p>Here we combine CloudBioLinux software with a CloudMan SGE cluster to build a fully automated pipeline for processing high throughput <a href="http://en.wikipedia.org/wiki/Exome_Sequencing">exome sequencing</a> data:</p>
<ul>
<li>The underlying analysis software is from CloudBioLinux.</li>
<li>CloudMan provides an SGE cluster managed via a web front end.</li>
<li><a href="http://www.rabbitmq.com/">RabbitMQ</a> is used for communication between cluster nodes.</li>
<li><a href="https://github.com/chapmanb/bcbb/tree/master/nextgen">An automated pipeline</a>, written in <a href="http://python.org">Python</a>, organizes parallel processing across the cluster.</li>
</ul>
<p>Below are instructions for starting a cluster on <a href="http://aws.amazon.com/ec2/">Amazon EC2</a> resources to run an exome sequencing pipeline that processes <a href="http://en.wikipedia.org/wiki/FASTQ_format">FASTQ</a> sequencing reads, producing fully annotated <a href="http://en.wikipedia.org/wiki/Variant_Call_Format">variant calls</a>.</p>
<div id="start-cluster-with-cloudbiolinux-and-cloudman">
<h2>Start cluster with CloudBioLinux and CloudMan</h2>
<p>Start in the <a href="https://console.aws.amazon.com/ec2/">Amazon web console</a>, a convenient front end for managing EC2 servers. The first step is to follow the <a href="http://wiki.g2.bx.psu.edu/Admin/Cloud">CloudMan setup instructions</a> to create an Amazon account and set up appropriate security groups and user data. The <a href="http://wiki.g2.bx.psu.edu/Admin/Cloud">wiki page</a> contains detailed screencasts. Below is a short screencast showing how to boot your CloudBioLinux specific CloudMan server:</p>
<p>[youtube=http://www.youtube.com/watch?v=eS8vmKIXIB4]</p>
<p></p>
<p>Once this is booted, proceed to the CloudMan web interface on the server and startup an instance from this shared identifier:</p>
<pre><code>cm-b53c6f1223f966914df347687f6fc818/shared/2012-07-23--19-23
</code></pre>
<p></p>
<p>This screencast shows all of the details, including starting an additional node on the SGE cluster:</p>
<p>[youtube=http://www.youtube.com/watch?v=4kIRI1m0g7Y]<br /></p>
</div>
<div id="configure-amqp-messaging">
<h2>Configure AMQP messaging</h2>
<p><b>Edit:</b> The AMQP messaging steps have now been full automated so the configuration steps in this section are no longer required. Skip down to the 'Run Analysis' section to start processing the data immediately.</p>
<p>With your server booted and ready to run, the next step is to configure RabbitMQ messaging to communicate between nodes on your cluster. In the AWS console, find the external and internal hostname of the head machine. Start by opening an ssh connection to the machine with the external hostname:</p>
<pre><code>$ ssh -i your-keypair ubuntu@ec2-50-19-177-134.compute-1.amazonaws.com
</code></pre>
<p></p>
<p>Edit the <code>/export/data/galaxy/universe_wsgi.ini</code> configuration file to add the internal hostname. After editing, the AMQP section will look like:</p>
<pre><code>[galaxy_amqp]
host = ip-10-125-10-182.ec2.internal
port = 5672
userid = biouser
password = tester
</code></pre>
<p></p>
<p>Finally, add the user and virtual host to the running RabbitMQ server on the master node with 3 commands:</p>
<pre><code>$ sudo rabbitmqctl add_user biouser tester
creating user &quot;biouser&quot; ...
...done.
$ sudo rabbitmqctl add_vhost bionextgen
creating vhost &quot;bionextgen&quot; ...
...done.
$ sudo rabbitmqctl set_permissions -p bionextgen biouser &quot;.*&quot; &quot;.*&quot; &quot;.*&quot;
setting permissions for user &quot;biouser&quot; in vhost &quot;bionextgen&quot; ...
...done.
</code></pre>
<p>
</div>
<div id="run-analysis">
<h2>Run analysis</h2>
<p>With messaging in place, we are ready to run the analysis. <code>/export/data</code> contains a ready to run example exome analysis, with <a href="http://en.wikipedia.org/wiki/FASTQ_format">FASTQ</a> input files in <code>/export/data/exome_example/fastq</code> and configuration information in <code>/export/data/exome_example/config</code>. Start the fully automated pipeline with a single command:</p>
<pre><code> $ cd /export/data/work
 $ distributed_nextgen_pipeline.py /export/data/galaxy/post_process.yaml
                                   /export/data/exome_example/fastq
                                   /export/data/exome_example/config/run_info.yaml
</code></pre>
<p></p>
<p><code>distributed_nextgen_pipeline.py</code> starts processing servers on each of the cluster nodes, using SGE for scheduling. Then a top level analysis server runs, splitting the FASTQ data across the nodes at each step of the process:</p>
<ul>
<li>Alignment with <a href="http://bio-bwa.sourceforge.net/">BWA</a></li>
<li>Preparation of merged alignment files with <a href="http://picard.sourceforge.net/">Picard</a></li>
<li>Recalibration and realignment with <a href="http://www.broadinstitute.org/gsa/wiki/index.php/The_Genome_Analysis_Toolkit">GATK</a></li>
<li>Variant calling with GATK</li>
<li>Assessment of predicted variant effects with <a href="http://snpeff.sourceforge.net/">snpEff</a></li>
<li>Preparation of summary PDFs for each sample with read details from <a href="http://www.bioinformatics.bbsrc.ac.uk/projects/fastqc/">FastQC</a> alongside alignment, hybrid selection and variant calling statistics from Picard</li>
</ul>
</div>
<div id="monitor-the-running-process">
<h2>Monitor the running process</h2>
<p>The example data is from a human chromosome 22 hybrid selection experiment. While running, you can keep track of the progress in several ways. SGEs <code>qstat</code> command will tell you where the analysis servers are running on the cluster:</p>
<pre><code>$ qstat
ob-ID  prior   name   user  state submit/start at   queue
----------------------------------------------------------------------------------
1 0.55500 nextgen_an ubuntu  r  08/14/2011 18:16:32 all.q@ip-10-125-10-182.ec2.int
2 0.55500 nextgen_an ubuntu  r  08/14/2011 18:16:32 all.q@ip-10-86-254-105.ec2.int
3 0.55500 automated_ ubuntu  r  08/14/2011 18:16:47 all.q@ip-10-125-10-182.ec2.int
</code></pre>
<p></p>
<p>Listing files in the working directory will show our progress:</p>
<pre><code>$ cd /export/data/work
$ ls -lh
drwxr-xr-x 2 ubuntu ubuntu 4.0K 2011-08-13 21:09 alignments
-rw-r--r-- 1 ubuntu ubuntu 2.0K 2011-08-13 21:17 automated_initial_analysis.py.o11
drwxr-xr-x 2 ubuntu ubuntu   33 2011-08-13 20:43 log
-rw-r--r-- 1 ubuntu ubuntu  15K 2011-08-13 21:17 nextgen_analysis_server.py.o10
-rw-r--r-- 1 ubuntu ubuntu  15K 2011-08-13 21:17 nextgen_analysis_server.py.o9
drwxr-xr-x 8 ubuntu ubuntu  102 2011-08-13 21:06 tmp
</code></pre>
<p></p>
<p>The files that end with <code>.o*</code> are log files from each of the analysis servers and provide detailed information about the current state of processing at each server:</p>
<pre><code>$ less nextgen_analysis_server.py.o10
INFO: nextgen_pipeline: Processing sample: Test replicate 2; lane
  8; reference genome hg19; researcher ; analysis method SNP calling
INFO: nextgen_pipeline: Aligning lane 8_100326_FC6107FAAXX with bwa aligner
INFO: nextgen_pipeline: Combining and preparing wig file [u'', u'Test replicate 2']
INFO: nextgen_pipeline: Recalibrating [u'', u'Test replicate 2'] with GATK
</code></pre>
<p></p>
</div>
<div id="retrieve-results">
<h2>Retrieve results</h2>
<p>The processing pipeline results in numerous intermediate files. These take up a lot of disk space and are not necessary after processing is finished. The final step in the process is to extract the useful files for visualization and further analysis:</p>
<pre><code>$ upload_to_galaxy.py /export/data/galaxy/post_process.yaml
                      /export/data/exome_example/fastq
                      /export/data/work
                      /export/data/exome_example/config/run_info.yaml
</code></pre>
<p></p>
<p>For each sample, this script copies:</p>
<ul>
<li>A <a href="http://samtools.sourceforge.net/SAM1.pdf">BAM file</a> with aligned sequeneces and original FASTQ data</li>
<li>A realigned and recalibrated BAM file, ready for variant calling</li>
<li>Variant calls in <a href="http://en.wikipedia.org/wiki/Variant_Call_Format">VCF format</a>.</li>
<li>A tab delimited file of predicted variant effects.</li>
<li>A PDF summary file containing alignment, variant calling and hybrid selection statistics.</li>
</ul>
<p>into an output directory for the flowcell: <code>/export/data/galaxy/storage/100326_FC6107FAAXX</code>:</p>
<pre><code>$ ls -lh /export/data/galaxy/storage/100326_FC6107FAAXX/7
-rw-r--r-- 1 ubuntu ubuntu  38M 2011-08-19 20:50 7_100326_FC6107FAAXX.bam
-rw-r--r-- 1 ubuntu ubuntu  22M 2011-08-19 20:50 7_100326_FC6107FAAXX-coverage.bigwig
-rw-r--r-- 1 ubuntu ubuntu  72M 2011-08-19 20:51 7_100326_FC6107FAAXX-gatkrecal.bam
-rw-r--r-- 1 ubuntu ubuntu 109K 2011-08-19 20:51 7_100326_FC6107FAAXX-snp-effects.tsv
-rw-r--r-- 1 ubuntu ubuntu 827K 2011-08-19 20:51 7_100326_FC6107FAAXX-snp-filter.vcf
-rw-r--r-- 1 ubuntu ubuntu 1.6M 2011-08-19 20:50 7_100326_FC6107FAAXX-summary.pd
</code></pre>
<p></p>
<p>As suggested by the name, the script can also integrate the data into a <a href="http://usegalaxy.org">Galaxy instance</a> if desired. This allows biologists to perform further data analysis, including visual inspection of the alignments in the <a href="http://genome.ucsc.edu/">UCSC browser</a>.</p>
</div>
<div id="learn-more">
<h2>Learn more</h2>
<p>All components of the pipeline are open source and part of community projects. CloudMan, CloudBioLinux and the pipeline are customized through <a href="http://en.wikipedia.org/wiki/YAML">YAML</a> configuration files. Combined with the CloudMan managed SGE cluster, the pipeline can be applied in parallel to any number of samples.</p>
<p>The overall goal is to share the automated infrastructure work that moves samples from sequencing to being ready for analysis. This allows biologists more rapid access to the processed data, focusing attention on the real work: answering scientific questions.</p>
<p>If you'd like to hear more about CloudBioLinux, CloudMan and the exome sequencing pipeline, I'll be discussing it at the <a href="http://aws.amazon.com/genomicsevent/">AWS Genomics Event</a> in Seattle on September 22nd.</p>
</div>
